{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23144/358178464.py:25: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  df_dict = df.to_dict()\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Getting the category and attributes from the excel file and matching it with the data from the json file.\n",
    "\"\"\"\n",
    "import json\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def post_process_attribute_df(excel_file_path):\n",
    "    df = pd.read_csv(excel_file_path)\n",
    "    df = df.transpose()\n",
    "    df.head()\n",
    "    df = df.drop(\"ID\")\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df.drop([\"ATTRIBUTE\"])\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.fillna(\"empty\", inplace = True)\n",
    "    return df\n",
    "\n",
    "\n",
    "with open('test.json', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "def df_to_dict(df):\n",
    "    category_dict = dict()\n",
    "    df_dict = df.to_dict()\n",
    "    for i in df_dict:\n",
    "        category_dict[i] = list(df_dict[i].values())\n",
    "    return category_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def post_process_category_dict(category_dict):\n",
    "    for i in category_dict:\n",
    "        category_dict[i] =  [x for x in category_dict[i] if x != 'empty']\n",
    "    return category_dict\n",
    "\n",
    "\n",
    "excel_file_path = 'NER_Attributes.csv'\n",
    "df=post_process_attribute_df(excel_file_path)\n",
    "\n",
    "category_dict = df_to_dict(df)\n",
    "category_dict = post_process_category_dict(category_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sohaib/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/sohaib/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import re\n",
    "import copy\n",
    "\n",
    "\n",
    "# download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english') + list(string.punctuation)\n",
    "\n",
    "def preprocess(text):\n",
    "    # tokenize text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # remove stopwords and punctuation\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # lemmatize words\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    # join tokens back into a string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    return preprocessed_text\n",
    "\n",
    "\n",
    "\n",
    "def extract_weight_and_length(text):\n",
    "    \"\"\"\n",
    "    Extracts weight and length values along with their units from text using regular expressions.\n",
    "    Returns a dictionary with keys 'weight' and 'length'.\n",
    "    \"\"\"\n",
    "    # regular expressions to detect weight and length values\n",
    "    weight_pattern = r\"\\b(\\d+(?:\\.\\d+)?)\\s*(g|kg|pound|lb)\\b\"\n",
    "    length_pattern = r\"\\b(\\d+(?:\\.\\d+)?)\\s*(m|cm|mm|kilometer|km|mile|inch|in)\\b\"\n",
    "    size_pattren =   r'\\b(\\d+\\s*x\\s*\\d+)\\s*(m|cm|mm|kilometer|km|mile|inch|in)\\b'\n",
    "\n",
    "\n",
    "    # extract weight and length values and their units from text\n",
    "    weights = re.findall(weight_pattern, text, re.IGNORECASE)\n",
    "    lengths = re.findall(length_pattern, text, re.IGNORECASE)\n",
    "    sizes = re.findall(size_pattren, text, re.IGNORECASE)\n",
    "\n",
    "    # convert weight values to grams\n",
    "    size_dict = {}\n",
    "    for size, unit in sizes:\n",
    "        size_dict['size'] = str(size) + str(unit)\n",
    "\n",
    "    # convert length values to meters\n",
    "    length_dict = {}\n",
    "    for length, unit in lengths:\n",
    "        \n",
    "        length_dict['length'] = str(length) + str(unit)\n",
    "\n",
    "    # combine weight and length dictionaries and return\n",
    "    result = {}\n",
    "    result.update(length_dict)\n",
    "    result.update(size_dict)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1416it [11:17,  2.35it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "final_output = {}\n",
    "\n",
    "def get_tags(data, category_dict):\n",
    "    response = []\n",
    "    final_json = dict()\n",
    "    for index, chunk in tqdm(enumerate(data)):\n",
    "        final_output = dict()\n",
    "        description = chunk['SKU_SHORT_DESCRIPTION'] + chunk['SKU_LONG_DESCRIPTION'] + chunk['SKU_TITLE']\n",
    "        description = preprocess(description)\n",
    "        final_json[\"description\"] = description\n",
    "        for category in category_dict:\n",
    "            values = category_dict[category]\n",
    "            words = list(set(description.split(\" \")))\n",
    "            words = [word.lower() for word in words]\n",
    "            for item in category_dict[category]:\n",
    "                item = str(item)\n",
    "                item = preprocess(item)\n",
    "                for word in words:\n",
    "\n",
    "                    match_ratio = fuzz.ratio(item, word)\n",
    "                    if ( match_ratio > 90):\n",
    "                        if category in final_output:\n",
    "                            final_output[category].append(item)\n",
    "                        else:\n",
    "                            final_output[category] = [item]\n",
    "        \n",
    "        sizes = extract_weight_and_length(description)\n",
    "        final_output.update(sizes)\n",
    "        final_json[\"tags\"] = final_output\n",
    "        response.append(copy.deepcopy(final_json))\n",
    "    return response\n",
    "\n",
    "resp = get_tags(data, category_dict)\n",
    "\n",
    "# \n",
    "# print(\"\\n\\nfinal output\", final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"result.json\", \"w\") as outfile:\n",
    "    json.dump(resp, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NER",
   "language": "python",
   "name": "ner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
